{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/chat_f16\\\\tokenizer_config.json',\n",
       " './models/chat_f16\\\\special_tokens_map.json',\n",
       " './models/chat_f16\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import safetensors.torch\n",
    "# 使用中文版\n",
    "model_id = \"./models/chat\"\n",
    "# 或者，使用原版\n",
    "# model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "model =  AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 将模型转换为 f16（半精度）\n",
    "model = model.half()\n",
    "\n",
    "# 保存转换后的模型\n",
    "output_dir = model_id+\"_f16\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralConfig {\n",
      "  \"_name_or_path\": \"models/chat_f16\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"head_dim\": 26,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 312,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1092,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 10,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 1024,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32002\n",
      "}\n",
      "\n",
      "Name: model.embed_tokens.weight, Size: torch.Size([32002, 312])\n",
      "Name: model.layers.0.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.0.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.0.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.0.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.0.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.0.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.0.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.0.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.0.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.1.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.1.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.1.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.1.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.1.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.1.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.1.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.1.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.1.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.2.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.2.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.2.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.2.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.2.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.2.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.2.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.2.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.2.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.3.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.3.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.3.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.3.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.3.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.3.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.3.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.3.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.3.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.4.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.4.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.4.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.4.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.4.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.4.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.4.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.4.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.4.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.5.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.5.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.5.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.5.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.5.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.5.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.5.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.5.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.5.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.6.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.6.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.6.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.6.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.6.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.6.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.6.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.6.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.6.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.7.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.7.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.7.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.7.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.7.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.7.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.7.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.7.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.7.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.8.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.8.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.8.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.8.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.8.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.8.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.8.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.8.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.8.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.9.self_attn.q_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.9.self_attn.k_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.9.self_attn.v_proj.weight, Size: torch.Size([104, 312])\n",
      "Name: model.layers.9.self_attn.o_proj.weight, Size: torch.Size([312, 312])\n",
      "Name: model.layers.9.mlp.gate_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.9.mlp.up_proj.weight, Size: torch.Size([1092, 312])\n",
      "Name: model.layers.9.mlp.down_proj.weight, Size: torch.Size([312, 1092])\n",
      "Name: model.layers.9.input_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.layers.9.post_attention_layernorm.weight, Size: torch.Size([312])\n",
      "Name: model.norm.weight, Size: torch.Size([312])\n",
      "Name: lm_head.weight, Size: torch.Size([32002, 312])\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaModel,AutoModelForCausalLM\n",
    "import torch\n",
    "# excute from project directory.\n",
    "model_directory = \"models/chat_f16\"\n",
    "\n",
    "model =AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}, Size: {param.size()}\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "# text = \"Once upon a time\"\n",
    "# inputs = tokenizer(text)\n",
    "# outputs_dict = {}\n",
    "\n",
    "# def hook_fn(layer_name):\n",
    "#     def hook(module, input, output):\n",
    "#         outputs_dict[layer_name] = {\n",
    "#             \"input\": input,\n",
    "#             \"output\": output\n",
    "#         }\n",
    "#     return hook\n",
    "\n",
    "    \n",
    "\n",
    "# # 注册钩子\n",
    "# for name, layer in model.named_modules():\n",
    "#     layer_name = f\"transformer_layer_{name}\"\n",
    "#     layer.register_forward_hook(hook_fn(layer_name))\n",
    "\n",
    "# # 执行推理\n",
    "# # with torch.no_grad():\n",
    "# #     print(model(**inputs))\n",
    "\n",
    "\n",
    "\n",
    "# for layer_name, data in outputs_dict.items():\n",
    "#     print(f\"Layer: {layer_name}\")\n",
    "#     if isinstance(data['input'], tuple):\n",
    "#         for t in data['input']:\n",
    "#             if isinstance(t , torch.Tensor):\n",
    "#                 print(f\"Input shape: {t.shape}\")\n",
    "#     else:\n",
    "#         print(f\"Input shape: {data['input'].shape}\")\n",
    "\n",
    "#     if isinstance(data['output'], tuple):\n",
    "#         for t in data['output']:\n",
    "#             if isinstance(t , torch.Tensor):\n",
    "#                 print(f\"Output shape: {t.shape}\")\n",
    "#     elif isinstance(data['output'], torch.Tensor):\n",
    "#         print(f\"Output shape: {data['output'].shape}\")\n",
    "#     else:\n",
    "#         print(f\"Output type: {type(t)}\")\n",
    "#     print(f\"Input: {data['input']}\")\n",
    "#     print(f\"Output: {data['output']}\")\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
